{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Modeling & Evaluation for Performance Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook evaluates various baseline image captioning models (Random, Most Common,etc.) to provide a performance benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 13:48:56.551750: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-20 13:48:56.569352: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753033736.592145   62355 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753033736.597656   62355 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753033736.607268   62355 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753033736.607285   62355 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753033736.607286   62355 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753033736.607288   62355 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-20 13:48:56.610384: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from vtt.data.caption_preprocessing import load_and_clean_captions\n",
    "from vtt.data.data_loader import load_split_datasets\n",
    "from vtt.data.image_preprocessing import load_features \n",
    "from vtt.evaluation.evaluate import evaluate_captions\n",
    "from vtt.baselines import (\n",
    "    generate_random_captions,\n",
    "    generate_most_common_caption,\n",
    "    generate_nearest_neighbor_captions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 13:49:35 - INFO: Notebook setup complete and logging configured.\n"
     ]
    }
   ],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(levelname)s: %(message)s', \n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Notebook setup complete and logging configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Paths and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 13:49:37 - INFO: Defining data paths.\n",
      "2025-07-20 13:49:37 - INFO: Finished defining paths.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Defining data paths.\")\n",
    "dataset_name = \"flickr8k\"\n",
    "\n",
    "features_path = f\"../data/processed/{dataset_name}_features.npz\"\n",
    "captions_path = f\"../data/processed/{dataset_name}_padded_caption_sequences.npz\"\n",
    "tokenizer_path = f\"../data/processed/{dataset_name}_tokenizer.json\"\n",
    "captions_file = f\"../data/raw/{dataset_name}_captions.csv\"\n",
    "logger.info(\"Finished defining paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 13:49:44 - INFO: Loading dataset splits and raw captions...\n",
      "2025-07-20 13:50:15 - INFO: Finished loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Dataset Split Sizes (number of individual samples) ---\n",
      "Total samples loaded: 38008\n",
      "Train samples: 28507\n",
      "Validation samples: 5701\n",
      "Test samples: 3800\n",
      "----------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data in numpy format to easily access image IDs, features, and captions\n",
    "logger.info(\"Loading dataset splits and raw captions...\")\n",
    "train_data_np, val_data_np, test_data_np = load_split_datasets(\n",
    "    features_path=features_path,\n",
    "    captions_path=captions_path,\n",
    "    batch_size=64, # Batch size not directly relevant for numpy return, but kept for consistency\n",
    "    val_split=0.15,\n",
    "    test_split=0.10,\n",
    "    shuffle=True,\n",
    "    buffer_size=1000,\n",
    "    seed=42,\n",
    "    cache=False, # No need to cache if returning numpy arrays\n",
    "    return_numpy=True # Crucial for accessing raw numpy arrays (features, captions, IDs)\n",
    ")\n",
    "\n",
    "# Unpack the numpy arrays for easier access\n",
    "train_features_all_samples, train_caption_seqs_all_samples, train_image_ids_all_samples = train_data_np\n",
    "test_features_all_samples, test_caption_seqs_all_samples, test_image_ids_all_samples = test_data_np\n",
    "\n",
    "logger.info(f\"Finished loading.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 13:51:19 - INFO: Loading and cleaning all raw captions (for references and pool)...\n",
      "2025-07-20 13:51:19 - INFO: Loading all unique image features...\n",
      "2025-07-20 13:51:26 - INFO: Prepared a pool of 37957 training captions for random and most common assignment.\n",
      "2025-07-20 13:51:26 - INFO: Total unique test images to evaluate: 3158.\n"
     ]
    }
   ],
   "source": [
    "# Load all raw (cleaned) captions for reference and pooling\n",
    "# This dictionary is {unique_image_id: [list of cleaned captions]}\n",
    "logger.info(\"Loading and cleaning all raw captions (for references and pool)...\")\n",
    "clean_captions_dict = load_and_clean_captions(captions_file)\n",
    "\n",
    "# Load pre-extracted image features as a dictionary for Nearest Neighbor baseline\n",
    "# This dictionary is {unique_image_id: single_feature_vector}\n",
    "logger.info(\"Loading all unique image features...\")\n",
    "all_unique_image_features_dict = load_features(features_path)\n",
    "\n",
    "# Create a flat pool of all training captions for random and most common baselines\n",
    "# This flattens the list of lists of captions for all UNIQUE training images.\n",
    "# We get unique train_image_ids first to ensure we don't duplicate captions if\n",
    "# train_image_ids_all_samples has duplicates due to multiple captions per image.\n",
    "unique_train_image_ids_for_pool = list(set(train_image_ids_all_samples))\n",
    "training_captions_pool = [\n",
    "    caption for img_id in unique_train_image_ids_for_pool\n",
    "    if img_id in clean_captions_dict\n",
    "    for caption in clean_captions_dict[img_id]\n",
    "]\n",
    "\n",
    "# if not training_captions_pool:\n",
    "#     logger.error(\"No training captions found for the random pool. Check data loading or paths.\")\n",
    "#     # You might want to halt execution or load a dummy pool here in a real scenario\n",
    "\n",
    "logger.info(f\"Prepared a pool of {len(training_captions_pool)} training captions for random and most common assignment.\")\n",
    "\n",
    "# Get unique test image IDs for evaluation (these are the images we need to caption)\n",
    "unique_test_image_ids = list(set(test_image_ids_all_samples))\n",
    "logger.info(f\"Total unique test images to evaluate: {len(unique_test_image_ids)}.\")\n",
    "\n",
    "results = {} # Dictionary to store evaluation scores for all baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Random Caption Model and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 13:52:11 - INFO: \n",
      "--- Running Random Caption Model ---\n",
      "/home/curtis/anaconda3/envs/northeastern/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-07-20 13:57:28 - INFO: Random Caption Model Results:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BLEU-1: 0.3955\n",
      "  BLEU-2: 0.1748\n",
      "  BLEU-3: 0.0778\n",
      "  BLEU-4: 0.0474\n",
      "  METEOR: 0.2580\n",
      "  BERTScore_P: 0.8735\n",
      "  BERTScore_R: 0.8733\n",
      "  BERTScore_F1: 0.8733\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"\\n--- Running Random Caption Model ---\")\n",
    "\n",
    "random_assignments = generate_random_captions(unique_test_image_ids, training_captions_pool)\n",
    "random_scores = evaluate_captions(clean_captions_dict, random_assignments)\n",
    "results['Random'] = random_scores\n",
    "\n",
    "logger.info(\"Random Caption Model Results:\")\n",
    "for metric, value in random_scores.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Most Common Caption Model and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 13:58:32 - INFO: \n",
      "--- Running Most Common Caption Model ---\n",
      "/home/curtis/anaconda3/envs/northeastern/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-07-20 14:01:04 - INFO: Most Common Caption Model Results:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BLEU-1: 0.4381\n",
      "  BLEU-2: 0.1852\n",
      "  BLEU-3: 0.0978\n",
      "  BLEU-4: 0.0659\n",
      "  METEOR: 0.2692\n",
      "  BERTScore_P: 0.9002\n",
      "  BERTScore_R: 0.8782\n",
      "  BERTScore_F1: 0.8890\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"\\n--- Running Most Common Caption Model ---\")\n",
    "\n",
    "most_common_assignments = generate_most_common_caption(unique_test_image_ids, training_captions_pool)\n",
    "most_common_scores = evaluate_captions(clean_captions_dict, most_common_assignments)\n",
    "results['Most Common'] = most_common_scores\n",
    "\n",
    "logger.info(\"Most Common Caption Model Results:\")\n",
    "for metric, value in most_common_scores.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Nearest Neighbor Image Caption Model and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 14:06:49 - INFO: \n",
      "--- Running Nearest Neighbor Image Caption Model (k=1) ---\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-07-20 14:11:55 - INFO: Nearest Neighbor (k=1) Model Results:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BLEU-1: 0.9975\n",
      "  BLEU-2: 0.9967\n",
      "  BLEU-3: 0.9963\n",
      "  BLEU-4: 0.9962\n",
      "  METEOR: 0.9966\n",
      "  BERTScore_P: 0.9260\n",
      "  BERTScore_R: 0.9253\n",
      "  BERTScore_F1: 0.9256\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"\\n--- Running Nearest Neighbor Image Caption Model (k=1) ---\")\n",
    "\n",
    "# Filter unique image IDs that actually have features loaded\n",
    "train_image_ids_for_nn = [img_id for img_id in unique_train_image_ids_for_pool if img_id in all_unique_image_features_dict]\n",
    "test_image_ids_for_nn = [img_id for img_id in unique_test_image_ids if img_id in all_unique_image_features_dict]\n",
    "\n",
    "# Create feature dictionaries containing only relevant (unique) image features\n",
    "train_features_for_nn = {\n",
    "    img_id: all_unique_image_features_dict[img_id]\n",
    "    for img_id in train_image_ids_for_nn\n",
    "}\n",
    "test_features_for_nn = {\n",
    "    img_id: all_unique_image_features_dict[img_id]\n",
    "    for img_id in test_image_ids_for_nn\n",
    "}\n",
    "\n",
    "# Define number of neighbors\n",
    "k = 1\n",
    "\n",
    "nn_assignments = generate_nearest_neighbor_captions(\n",
    "    test_image_ids_for_nn,\n",
    "    train_image_ids_for_nn,\n",
    "    train_features_for_nn,\n",
    "    test_features_for_nn,\n",
    "    clean_captions_dict,\n",
    "    k_neighbors=k\n",
    ")\n",
    "nn_scores = evaluate_captions(clean_captions_dict, nn_assignments)\n",
    "results[f'Nearest Neighbor (k={k})'] = nn_scores\n",
    "\n",
    "logger.info(f\"Nearest Neighbor (k={k}) Model Results:\")\n",
    "for metric, value in nn_scores.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 14:15:55 - INFO: \n",
      "--- Baseline Model Performance Summary --- \n",
      "/tmp/ipykernel_62355/1573379827.py:16: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  results_df_formatted = results_df.applymap(lambda x: f\"{x:.4f}\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BLEU-1</th>\n",
       "      <th>BLEU-2</th>\n",
       "      <th>BLEU-3</th>\n",
       "      <th>BLEU-4</th>\n",
       "      <th>METEOR</th>\n",
       "      <th>BERTScore_P</th>\n",
       "      <th>BERTScore_R</th>\n",
       "      <th>BERTScore_F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Random</th>\n",
       "      <td>0.3955</td>\n",
       "      <td>0.1748</td>\n",
       "      <td>0.0778</td>\n",
       "      <td>0.0474</td>\n",
       "      <td>0.2580</td>\n",
       "      <td>0.8735</td>\n",
       "      <td>0.8733</td>\n",
       "      <td>0.8733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Most Common</th>\n",
       "      <td>0.4381</td>\n",
       "      <td>0.1852</td>\n",
       "      <td>0.0978</td>\n",
       "      <td>0.0659</td>\n",
       "      <td>0.2692</td>\n",
       "      <td>0.9002</td>\n",
       "      <td>0.8782</td>\n",
       "      <td>0.8890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nearest Neighbor (k=1)</th>\n",
       "      <td>0.9975</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>0.9963</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>0.9966</td>\n",
       "      <td>0.9260</td>\n",
       "      <td>0.9253</td>\n",
       "      <td>0.9256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        BLEU-1  BLEU-2  BLEU-3  BLEU-4  METEOR BERTScore_P  \\\n",
       "Random                  0.3955  0.1748  0.0778  0.0474  0.2580      0.8735   \n",
       "Most Common             0.4381  0.1852  0.0978  0.0659  0.2692      0.9002   \n",
       "Nearest Neighbor (k=1)  0.9975  0.9967  0.9963  0.9962  0.9966      0.9260   \n",
       "\n",
       "                       BERTScore_R BERTScore_F1  \n",
       "Random                      0.8733       0.8733  \n",
       "Most Common                 0.8782       0.8890  \n",
       "Nearest Neighbor (k=1)      0.9253       0.9256  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.info(\"\\n--- Baseline Model Performance Summary --- \")\n",
    "\n",
    "results_df = pd.DataFrame(results).T # Transpose to have models as rows\n",
    "\n",
    "# Define preferred order of metrics for display\n",
    "ordered_columns = [\n",
    "    'BLEU-1', 'BLEU-2', 'BLEU-3', 'BLEU-4',\n",
    "    'METEOR',\n",
    "    'BERTScore_P', 'BERTScore_R', 'BERTScore_F1'\n",
    "]\n",
    "# Filter to only include columns that actually exist in the DataFrame\n",
    "final_columns = [col for col in ordered_columns if col in results_df.columns]\n",
    "results_df = results_df[final_columns]\n",
    "\n",
    "# Format numbers for display\n",
    "results_df_formatted = results_df.applymap(lambda x: f\"{x:.4f}\")\n",
    "results_df_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 14:15:14 - INFO: \n",
      "--- Baseline Model Performance Summary --- \n",
      "/tmp/ipykernel_62355/99874289.py:16: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  results_df_formatted = results_df.applymap(lambda x: f\"{x:.4f}\")\n",
      "2025-07-20 14:15:14 - INFO: \n",
      "--- Baseline Model Evaluation Finished ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                        | BLEU-1   | BLEU-2   | BLEU-3   | BLEU-4   | METEOR   | BERTScore_P   | BERTScore_R   | BERTScore_F1   |\n",
      "|:-----------------------|:---------|:---------|:---------|:---------|:---------|:--------------|:--------------|:---------------|\n",
      "| Random                 | 0.3955   | 0.1748   | 0.0778   | 0.0474   | 0.258    | 0.8735        | 0.8733        | 0.8733         |\n",
      "| Most Common            | 0.4381   | 0.1852   | 0.0978   | 0.0659   | 0.2692   | 0.9002        | 0.8782        | 0.889          |\n",
      "| Nearest Neighbor (k=1) | 0.9975   | 0.9967   | 0.9963   | 0.9962   | 0.9966   | 0.926         | 0.9253        | 0.9256         |\n"
     ]
    }
   ],
   "source": [
    "# Print as Markdown table (good for copying to reports)\n",
    "print(results_df_formatted.to_markdown(numalign=\"left\", stralign=\"left\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Random and Most Common: These are solid, realistic lower bounds for a generative image captioning model. Any successful generative model should significantly outperform these.\n",
    "* Nearest Neighbor (k=1): While a valid baseline, its interpretation is crucial. These scores are exceptionally high, nearing perfection, and are highly unusual for a typical generative captioning task.\n",
    "    * What this likely means:\n",
    "        * Retrieval, not Generation: This baseline is effectively acting as a retrieval system, not a generative one. For each test image, it's finding the most visually similar image in the training set and then assigning one of the ground truth reference captions associated with that training image.\n",
    "        * After verifying the data splitting process is not allowing data leakage and there's no direct overlap or duplicates, the most likely reason for such near-perfect scores is that the test images are extremely similar to the training images, which leads to very similar image feature vectors. Visual inspection of the images shows this to be true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
