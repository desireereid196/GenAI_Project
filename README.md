<h1>
    <img src="./vtt_logo.png" alt="GHLightLogo" align="left" alt="Sample Image" class="image-left" width="80" height="80" style="padding: 10px;"/>
    Vision-to-Text: An Image Captioning System
</h1>
<br>

## ğŸ” Overview

This project bridges **Computer Vision** and **Natural Language Processing**, focusing on:

- Encoder-decoder architecture for image captioning
- Transfer learning with pretrained CNNs
- LSTM-based language modeling
- Embedding-based and generative evaluation metrics

## ğŸ“Œ Description

**VTT (Vision-to-Text)** is a modular deep learning pipeline for image captioning that translates visual content into coherent, semantically meaningful natural language descriptions. It combines computer vision (CV) and natural language processing (NLP) techniques via an encoder-decoder architecture using ResNet-50 and LSTM networks.

This project supports both the **[Flickr8k](https://www.kaggle.com/datasets/adityajn105/flickr8k)** and **[Flickr30k](https://www.kaggle.com/datasets/awsaf49/flickr30k-dataset)** datasets and includes tools for:
- Image preprocessing & feature extraction
- Caption tokenization, filtering, and padding
- Training-ready sequence generation
- Metric-based and qualitative evaluation


## ğŸ“ Repository Structure

```bash
GenAI_Project/
â”œâ”€â”€ LICENSE                           # MIT License.
â”œâ”€â”€ README.md                         # Repository overview and setup.
â”œâ”€â”€ pyproject.toml                    # Project configuration file.
â”œâ”€â”€ archive                           # Old stuff.
â”œâ”€â”€ data/                             # 
    â”œâ”€â”€ raw/                          # Raw data
    â””â”€â”€ processed/                    # Cleaned and processed data; tokenizers.
â”œâ”€â”€ documents/                        # Project milestones, research notes, etc.
â”œâ”€â”€ figures/                          # Performance plots.
â”œâ”€â”€ models/                           # Trained models.
â”œâ”€â”€ notebooks/                        # Development and experiment notebooks.
    â”œâ”€â”€ experiment_1.ipynb            # Baseline Training and Evaluation. (Research Question 1) 
    â”œâ”€â”€ experiment_2.ipynb            # Error Analysis. (Research Questions 2 and 4)
    â”œâ”€â”€ experiment_3.ipynb            # Semantic Fidelity Comparison. (Research Question 3)
    â””â”€â”€ experiment_4.ipynb            # Generalization. (Research Question 1)
â”œâ”€â”€ outputs/                          # Model runner outputs needed for Milestone 3.
â”œâ”€â”€ scripts/                          # 
    â”œâ”€â”€ data_runner.py                # Data pipeline script needed for Milestone 2.
    â”œâ”€â”€ model_runner.py               # Model pipeline script needed for Milestone 3.
    â”œâ”€â”€ train_model.py                # Model training script.
    â”œâ”€â”€ preprocess_captions.py        # Caption preprocessing script.
    â””â”€â”€ extract_features.py           # Feature extraction script.
â””â”€â”€ src/                              # Contains the core source code.
    â””â”€â”€ vtt/                          # The main package for the project.
        â”œâ”€â”€ __init__.py               #
        â”œâ”€â”€ config.py                 # Configuration file for project.
        â”œâ”€â”€ utils.py                  # Shared helper and utility functions.
        â”œâ”€â”€ captions/                 #
            â”œâ”€â”€ __init__.py           #
            â”œâ”€â”€ cleaning.py           # Load and clean captions.
            â”œâ”€â”€ vocabulary.py         # Count word frequencies and filter captions.
            â”œâ”€â”€ tokenization.py       # Fit tokenizer, convert captions to sequences, etc.
            â”œâ”€â”€ padding.py            # Pad caption sequences.
            â””â”€â”€ io.py                 # Save and load padded sequences.
        â””â”€â”€ features/                 #
            â”œâ”€â”€ __init__.py           #
            â”œâ”€â”€ preprocessing.py      # Image preprocessing.
            â”œâ”€â”€ extractor.py          # ResNet feature extraction.
            â””â”€â”€ batch_runner.py       # Batch processing and saving.
        â”œâ”€â”€ models/                   #
            â”œâ”€â”€ __init__.py           #
            â”œâ”€â”€ architecture.py       # Model building.
            â”œâ”€â”€ data_loader.py        # Data pipeline setup.
            â””â”€â”€ trainer.py            # Training orchestration.

```

## ğŸ›  Setup

Follow the steps below to set up the project locally for development, experimentation, or training.

### 1. Clone the Repository

```bash
git clone https://github.com/<your-username>/GenAi_Project.git
cd GenAI_Project
```

### 2. Create a Virtual Environment 

We recommend using Python 3.10+ with TensorFlow 2.9+ support.

```bash
conda create -n genai_project python=3.10
conda activate genai_project
```

### 3. Install the Package and Dependencies

Install the `vtt` package in editable (`-e`) mode so you can make changes to the source code and test them without reinstalling.

```bash
# Ensure you are at the top-level of the GenAI_Project repository
pip install -e .
```

This installs the package locally while keeping it linked to the source code.

### 4. Verify the Installation

Verify that the `vtt` package was installed and is importable.

```bash
# Ensure your virtual environment is active
python -c "import vtt; print('vtt imported successfully')"
```

## ğŸ“„ License
MIT License â€” feel free to use, share, and modify.

## ğŸ¤ Contributing
Pull requests welcome! For major changes, please open an issue first to discuss what youâ€™d like to change.

## ğŸ§  Project Maintainers
- [Curtis Neiderer](mailto:neiderer.c@northeastern.edu)
- [Divya Maheshkumar](maheshkumar.d@northeastern.edu)
- [Desiree Reed](reed.des@northeastern.edu)
- [Minal Ahir](ahir.m@northeastern.edu")
- [Arundhati Ubhad]("ubhad.a@northeastern.edu")
- Contributors welcome!